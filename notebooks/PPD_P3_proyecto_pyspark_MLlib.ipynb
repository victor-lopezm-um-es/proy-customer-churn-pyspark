{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>\n",
        "  Predicción de Churn de Clientes con MLlib de PySpark\n",
        "</h1>\n",
        "\n",
        "Una agencia de marketing ofrece a muchas compañías un servicio para producir anuncios en páginas webs. La agencia ha detectado que muchas de estas compañías acaban dándose de baja del servicio, por lo que interesa mejorar su índice de retención.\n",
        "\n",
        "Para ello, el personal de la agencia ha pensado en aplicar algoritmos de Machine Learning que predigan los clientes que acabarán prescindiendo de sus servicios. De esta forma, podrán identificar cuáles son las compañías a las que les deberían asignar un ejecutivo de cuentas (Account Manager) para evitar que se marchen.\n",
        "\n"
      ],
      "metadata": {
        "id": "PQiXmgHgN12B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fZl6bH3d9wbr"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "# Elegir el máster de Spark dependiendo de si se ha definido la variable de entorno HADOOP_CONF_DIR o YARN_CONF_DIR\n",
        "SPARK_MASTER: str = 'yarn' if 'HADOOP_CONF_DIR' in os.environ or 'YARN_CONF_DIR' in os.environ else 'local[*]'\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('MLlibNaïveBayesProject') \\\n",
        "    .master(SPARK_MASTER) \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.read.csv('customer_churn.csv', header = True, inferSchema=True)"
      ],
      "metadata": {
        "id": "LXATOisV-boR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forzamos la redistribución del DataFrame en más particiones\n",
        "data = data.repartition(2)\n",
        "print(\"Número de particiones:\", data.rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ20iGLBK7fa",
        "outputId": "4ee889f8-8793-4f17-9059-156a06e02174"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de particiones: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este proyecto usaremos modelos supervisados. Como nuestro DataFrame estará dividido en varias particiones necesitamos barajar los registros. Así evitaremos que todos los registros de una clase acaben en una única partición."
      ],
      "metadata": {
        "id": "poYC9RiTSifd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Equivalente a una función shuffle()\n",
        "data = data.orderBy(F.rand())"
      ],
      "metadata": {
        "id": "YSqPfkjwSgoS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJDn12GGN3VP",
        "outputId": "84237c20-eb36-4384-d5ca-7647a5ff455b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, Names: string, Age: string, Total_Purchase: string, Account_Manager: string, Years: string, Num_Sites: string, Location: string, Company: string, Churn: string]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Información de las columnas\n",
        "\n",
        "**Name**: Nombre del último contacto de la compañía\n",
        "\n",
        "**Age**: Edad del cliente\n",
        "\n",
        "**Total_Purchase**: Número de anuncios comprados\n",
        "\n",
        "**Account_Manager**: 0 sin manager, 1 con manager asignado\n",
        "\n",
        "**Years**: Número de años como cliente\n",
        "\n",
        "**Num_sites**: Número de sitios web usando el servicio\n",
        "\n",
        "**Onboard_date**: Fecha en la que el contacto más reciente de una empresa fue registrado en el sistema.\n",
        "\n",
        "**Location**: Dirección de la sede central de la compañía.\n",
        "\n",
        "**Company**: Nombre de la compañía.\n",
        "\n",
        "**Churn**: Si el cliente se da de baja del servicio\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wgEoBp6TBxdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimimos el esquema del dataframe\n",
        "data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NN-AC4A_SNu",
        "outputId": "f26a8987-8d3f-4e6e-9891-b09dfd731757"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Names: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- Total_Purchase: double (nullable = true)\n",
            " |-- Account_Manager: integer (nullable = true)\n",
            " |-- Years: double (nullable = true)\n",
            " |-- Num_Sites: double (nullable = true)\n",
            " |-- Onboard_date: timestamp (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Company: string (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcoP5434-pgD",
        "outputId": "b9a9c13c-24f9-4e09-cd2c-cdb72a4cb62d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Names='Tammy Pope', Age=45.0, Total_Purchase=9576.07, Account_Manager=1, Years=7.01, Num_Sites=8.0, Onboard_date=datetime.datetime(2007, 6, 7, 6, 17, 8), Location='9490 Kathryn Tunnel Suite 480 Shaneland, GA 26165-9694', Company='Hays, Henson and Shelton', Churn=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Comprobamos cuantos clientes han abandonado y cuantos han seguido\n",
        "data.groupBy(col('Churn')).count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDQYmwUUd9UV",
        "outputId": "de74f196-a59a-4fa9-f181-aa3a87493a20"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|Churn|count|\n",
            "+-----+-----+\n",
            "|    1|  150|\n",
            "|    0|  750|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¡IMPORTANTE!**\n",
        "\n",
        "El dataset está desbalanceado. Será necesario aplicar algún tipo de estratificación para compensar la falta de representación de clientes que abandonan el servicio.\n",
        "\n",
        "Los dataframe tienen implementada una función RandomSplit para separar el conjunto de datos de entrenamiento del de test. Sin embargo, no asegura que exista la misma representación de etiquetas en ambos conjuntos. Por ello, vamos a implementar nuestra propia función que segregue los datos."
      ],
      "metadata": {
        "id": "mfdY9b9Jez7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "def stratified_split(df, label_col, train_frac=0.8, seed=42):\n",
        "    \"\"\"\n",
        "    Realiza una partición estratificada en conjuntos de entrenamiento y prueba.\n",
        "\n",
        "    Argumentos:\n",
        "        df (DataFrame): El DataFrame original.\n",
        "        label_col (str): Nombre de la columna de clase (por ejemplo, \"Churn\").\n",
        "        train_frac (float): Fracción de datos que irá al conjunto de entrenamiento.\n",
        "        seed (int): Semilla para la aleatoriedad.\n",
        "\n",
        "    Devuelve:\n",
        "        Tuple[DataFrame, DataFrame]: (train_df, test_df)\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtenemos los valores únicos de la clase (por ejemplo 0 y 1)\n",
        "    labels = [row[label_col] for row in df.select(label_col).distinct().collect()]\n",
        "\n",
        "    # Listas donde iremos guardando los datos segregados\n",
        "    train_parts = []\n",
        "    test_parts = []\n",
        "\n",
        "    for label in labels:\n",
        "        # Filtramos por etiqueta\n",
        "        subset = df.filter(col(label_col) == label)\n",
        "\n",
        "        # Del conjunto del dataframe filtrado muestreamos una proporción -> conjunto train\n",
        "        train_subset = subset.sample(withReplacement=False, fraction=train_frac, seed=seed)\n",
        "\n",
        "        # El resto del conjunto filtrado va para test\n",
        "        test_subset = subset.subtract(train_subset)\n",
        "\n",
        "        train_parts.append(train_subset)\n",
        "        test_parts.append(test_subset)\n",
        "\n",
        "    # Unimos los subconjuntos estratificados\n",
        "    train_df = train_parts[0].unionByName(train_parts[1])\n",
        "    test_df = test_parts[0].unionByName(test_parts[1])\n",
        "\n",
        "    return train_df, test_df\n"
      ],
      "metadata": {
        "id": "HD0JA9F5hQ4R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformación de la columna `Onboard_date`\n",
        "\n",
        "La columna almacena la fecha en la que se registró el contacto. Sería más interesante calcular el tiempo que ha pasado desde entonces, porque podríamos establecer una relación entre esta cantidad de tiempo con el hecho de si el cliente abandona el servicio."
      ],
      "metadata": {
        "id": "ZF_aGjsxoG5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.select('Onboard_date').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeDziPSXoELs",
        "outputId": "5a4f905f-866e-4e2a-cf0d-3fdc53d74f05"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|       Onboard_date|\n",
            "+-------------------+\n",
            "|2015-09-04 11:32:49|\n",
            "|2009-09-21 12:50:42|\n",
            "|2014-10-11 14:08:10|\n",
            "|2016-07-29 08:39:49|\n",
            "|2013-02-14 03:05:33|\n",
            "|2011-09-01 16:47:27|\n",
            "|2014-06-20 05:10:09|\n",
            "|2009-04-13 22:54:21|\n",
            "|2006-11-15 14:58:19|\n",
            "|2007-11-25 17:41:30|\n",
            "|2016-03-05 13:19:04|\n",
            "|2012-03-26 06:23:51|\n",
            "|2010-04-23 00:50:01|\n",
            "|2010-04-21 15:26:30|\n",
            "|2008-03-25 15:58:18|\n",
            "|2006-08-15 20:48:58|\n",
            "|2014-02-17 08:26:10|\n",
            "|2012-08-02 14:55:41|\n",
            "|2011-05-07 04:27:00|\n",
            "|2010-11-05 09:22:13|\n",
            "+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "colOnboard = col('Onboard_date')\n",
        "\n",
        "today_date = datetime.datetime.today()\n",
        "difDayfn = F.udf(lambda x: (today_date-x).days)\n",
        "data = data.withColumn('Days_Since_Last_Contact', difDayfn(colOnboard).cast('int'))\n",
        "data.select('Days_Since_Last_Contact').printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgYdT-kypMbY",
        "outputId": "6552cba5-b817-4951-aed1-227f883fabc4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Days_Since_Last_Contact: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocesamiento de datos\n",
        "A nuestro algoritmo Naïve Bayes de la librería MLlib solo le podemos aportar los predictores a través de una única columna (no 6 columnas). Es por ello que usaremos el transformador VectorAssembler, el cual reunirá la información de las 5 columnas en una columna que almacena vectores.\n",
        "\n",
        "Las variables que usaremos para predecir si un cliente prescinde de los servicios de la agencia serán `Age`, `Total_Purchase`, `Account_Manager`, `Years` y `Num_Sites`. Todas son numéricas, a excepción de `Account_Manager`. Aunque `Account_Manager` no sigue una distribución normal si no Bernoulli, voy a probar experimentalmente si incluirla en nuestra clasificador Naïve Bayes Gaussiano mejora los resultados de la clasificación."
      ],
      "metadata": {
        "id": "0c7JjFDlNEsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Renombramos la variable Churn como label (la etiqueta que queremos predecir)\n",
        "data = data.withColumnRenamed(\"Churn\", \"label\")"
      ],
      "metadata": {
        "id": "KF0aGYva_4SQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "feature_cols = ['Age', 'Total_Purchase', 'Years', 'Num_Sites', 'Days_Since_Last_Contact']\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
        "data_transformed = assembler.transform(data)"
      ],
      "metadata": {
        "id": "fhj9FVTHM5k6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transformed.select('features_vec').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMwUznPeAJbN",
        "outputId": "16026a98-1ca8-496b-cad9-01319c6ec41b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------+\n",
            "|features_vec                    |\n",
            "+--------------------------------+\n",
            "|[34.0,9779.12,5.89,9.0,5675.0]  |\n",
            "|[54.0,8092.55,4.55,10.0,4369.0] |\n",
            "|[34.0,7324.32,6.69,10.0,3774.0] |\n",
            "|[46.0,14664.0,6.54,8.0,4581.0]  |\n",
            "|[51.0,6114.85,4.7,9.0,4175.0]   |\n",
            "|[45.0,9034.21,6.89,9.0,5049.0]  |\n",
            "|[41.0,9552.57,3.81,8.0,3723.0]  |\n",
            "|[50.0,13247.31,3.48,6.0,5408.0] |\n",
            "|[48.0,10963.5,5.89,9.0,6249.0]  |\n",
            "|[45.0,9951.97,4.4,6.0,3667.0]   |\n",
            "|[51.0,12780.22,6.06,7.0,3665.0] |\n",
            "|[46.0,13725.55,5.09,9.0,6983.0] |\n",
            "|[44.0,12155.91,4.11,5.0,5738.0] |\n",
            "|[36.0,8972.54,5.23,8.0,5783.0]  |\n",
            "|[48.0,11316.41,4.74,10.0,6115.0]|\n",
            "|[35.0,10801.37,5.64,11.0,3786.0]|\n",
            "|[47.0,7222.35,6.41,11.0,3581.0] |\n",
            "|[32.0,10716.75,5.12,8.0,6775.0] |\n",
            "|[45.0,11486.53,5.5,7.0,5608.0]  |\n",
            "|[54.0,13184.51,6.04,8.0,4409.0] |\n",
            "+--------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_transformed.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCXYsSVqNYUx",
        "outputId": "6c411727-be66-4e6b-b6a6-9b44db6dbd71"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Names: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- Total_Purchase: double (nullable = true)\n",
            " |-- Account_Manager: integer (nullable = true)\n",
            " |-- Years: double (nullable = true)\n",
            " |-- Num_Sites: double (nullable = true)\n",
            " |-- Onboard_date: timestamp (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Company: string (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            " |-- Days_Since_Last_Contact: integer (nullable = true)\n",
            " |-- features_vec: vector (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segregación de datos"
      ],
      "metadata": {
        "id": "TtIhZdliNWJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = stratified_split(data_transformed, label_col='label', seed=42)"
      ],
      "metadata": {
        "id": "f468LHLfN5EQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación y entrenamiento del modelo"
      ],
      "metadata": {
        "id": "5cthlIEQOSgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "\n",
        "nb = NaiveBayes(featuresCol='features_vec', labelCol='label', modelType=\"gaussian\")\n",
        "model = nb.fit(train_data)\n"
      ],
      "metadata": {
        "id": "y5CsZXsYORiQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación del modelo"
      ],
      "metadata": {
        "id": "s6Z-hASdOepM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-0OkJVLOgoR",
        "outputId": "5bf93288-2627-41b3-920b-3f7d0ec015e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9172413793103448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ahora probamos incluyendo `Account_Manager`"
      ],
      "metadata": {
        "id": "sz_5EpblKCKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incluimos la variable binaria Account_Manager\n",
        "feature_cols = ['Age', 'Total_Purchase', 'Account_Manager', 'Years', 'Num_Sites', 'Days_Since_Last_Contact']\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
        "\n",
        "data_transformed = assembler.transform(data)\n",
        "data_transformed.select('features_vec').show(truncate=False)\n",
        "\n",
        "# Segregamos los datos en un conjunto de entrenamiento y test\n",
        "train_data, test_data = stratified_split(data_transformed, label_col='label', seed=42)\n",
        "\n",
        "# Entrenamos el modelo Naïve Bayes\n",
        "nb = NaiveBayes(featuresCol='features_vec', labelCol='label', modelType=\"gaussian\")\n",
        "model2 = nb.fit(train_data)\n",
        "\n",
        "# Realizamos las predicciones\n",
        "predictions = model2.transform(test_data)\n",
        "\n",
        "# Evaluamos el modelo\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vh14RWQJ5Qm",
        "outputId": "30a7695e-ba8a-48b1-e400-8453cfd23c6b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------+\n",
            "|features_vec                        |\n",
            "+------------------------------------+\n",
            "|[45.0,8595.24,0.0,6.4,10.0,4542.0]  |\n",
            "|[32.0,9472.72,1.0,1.0,10.0,5865.0]  |\n",
            "|[41.0,7777.37,0.0,4.81,12.0,4104.0] |\n",
            "|[45.0,7351.38,0.0,5.76,11.0,3113.0] |\n",
            "|[42.0,9635.87,1.0,2.43,9.0,4145.0]  |\n",
            "|[39.0,13532.85,1.0,5.63,9.0,5268.0] |\n",
            "|[47.0,13359.2,1.0,4.55,8.0,3183.0]  |\n",
            "|[45.0,11170.06,1.0,7.76,10.0,5193.0]|\n",
            "|[48.0,10241.32,1.0,7.25,7.0,3822.0] |\n",
            "|[44.0,9723.71,0.0,4.34,9.0,4673.0]  |\n",
            "|[50.0,10850.78,1.0,5.63,9.0,5416.0] |\n",
            "|[32.0,9036.27,0.0,7.14,11.0,5405.0] |\n",
            "|[43.0,6065.64,1.0,4.86,7.0,5194.0]  |\n",
            "|[49.0,6283.67,0.0,6.35,11.0,6674.0] |\n",
            "|[39.0,10741.9,0.0,5.48,8.0,4421.0]  |\n",
            "|[34.0,8772.26,0.0,5.78,9.0,4079.0]  |\n",
            "|[44.0,11331.58,1.0,5.23,11.0,3054.0]|\n",
            "|[48.0,9722.23,1.0,5.98,6.0,4133.0]  |\n",
            "|[47.0,7364.12,1.0,3.65,9.0,4849.0]  |\n",
            "|[55.0,8907.17,1.0,7.6,11.0,3386.0]  |\n",
            "+------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Accuracy: 0.9103448275862069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, añadir la variable binaria no ha supuesto ninguna mejora de la calidad del modelo. Lo cierto es que usar un modelo Naïve Bayes suele funcionar bien cuando los tipos de datos son homogéneos. En su lugar, podríamos optar por usar un modelo de regresión logística, el cual sí tolera esta hetoregeneidad en los datos."
      ],
      "metadata": {
        "id": "C_e5svagMJk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso del modelo de regresión logística"
      ],
      "metadata": {
        "id": "wvdgCVFZO1l4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta vez plantearemos el modelo a partir de un pipeline. Los stages estarán formados por el VectorAssembler, el StandardScaler y el modelo de regresión logística."
      ],
      "metadata": {
        "id": "n_kAV7sCxKOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escalamos los datos\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features_scaled\")"
      ],
      "metadata": {
        "id": "gGrgOdO7LR6g"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el modelo de regresión logística\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "logReg = LogisticRegression(featuresCol=\"features_scaled\", labelCol=\"label\")\n"
      ],
      "metadata": {
        "id": "990cSxghX1wa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages=[scaler,logReg])"
      ],
      "metadata": {
        "id": "remFc2ZxYXRE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Realizamos las predicciones\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluamos el modelo\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTUg4MkgYyeE",
        "outputId": "67c76405-e7f3-4241-aa40-af7541d08435"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9103448275862069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión**\n",
        "\n",
        "El modelo Gaussiano de Naïve Bayes nos ha aportado un mayor rendimiento que el modelo de regresión logística. Esto podría deberser al gran número de variables numéricas con las que contaba nuestro dataset, aunque algunas fueran discretas y binarias."
      ],
      "metadata": {
        "id": "dPnvc6MW0RM0"
      }
    }
  ]
}